{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "from scipy.sparse import csr_matrix, csc_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Étape 1 : Création du dataframe\n",
        "def load_csv_to_dataframe(file_path):\n",
        "    return pd.read_csv(file_path)\n",
        "df_avant_traitement = load_csv_to_dataframe('logs_corrompu_label.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Étape 2 : Création du job de prétraitement\n",
        "def preprocess_data(file_path, save_job=True, job_file='preprocessing_pipeline.joblib'):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # a. Enlever toutes les lignes où la Date est vide\n",
        "    df = df.dropna(subset=['Date'])\n",
        "\n",
        "    # b. Affecter à tous les Process \"kernel\" la valeur 10 à l'IdProcess quand il est NaN\n",
        "    # df.loc[(df['Process'] == 'kernel') & (df['IdProcess'].isna()), 'IdProcess'] = 10\n",
        "\n",
        "    # c. Limiter les colonnes à encoder\n",
        "    max_unique_values = 1000\n",
        "    label_column = 'Label'\n",
        "    columns_to_encode = [col for col in df.columns.difference([label_column]) \n",
        "                         if df[col].nunique() <= max_unique_values]\n",
        "\n",
        "    # d. Appliquer un OneHotEncoding sparse\n",
        "    encoder = OneHotEncoder(sparse=True, handle_unknown='ignore')\n",
        "    encoded_features = encoder.fit_transform(df[columns_to_encode])\n",
        "    df_encode = pd.DataFrame.sparse.from_spmatrix(encoded_features, \n",
        "                                                  columns=encoder.get_feature_names_out(columns_to_encode))\n",
        "\n",
        "    # e. Ajouter la colonne Label\n",
        "    if label_column in df.columns:\n",
        "        df_encode[label_column] = df[label_column].values\n",
        "\n",
        "    # f. Sauvegarder le job de prétraitement\n",
        "    if save_job:\n",
        "        joblib.dump(encoder, job_file)\n",
        "\n",
        "    return df_encode\n",
        "\n",
        "df_encode = preprocess_data('logs_corrompu_label.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Étape 3 : Entraînement du modèle avec RandomForest\n",
        "def train_model_without_smote(df_encoded, label_column='Label', save_model=True, model_file='random_forest_model.joblib'):\n",
        "    # Vérifier les valeurs manquantes dans la colonne cible\n",
        "    if df_encoded[label_column].isna().sum() > 0:\n",
        "        print(f\"Attention : {df_encoded[label_column].isna().sum()} valeurs manquantes dans '{label_column}'.\")\n",
        "        df_encoded = df_encoded.dropna(subset=[label_column])  # Supprimer les lignes avec Label manquant\n",
        "\n",
        "    X = df_encoded.drop(columns=[label_column])\n",
        "    y = df_encoded[label_column]\n",
        "\n",
        "    # Vérifier les types des données (convertir au besoin)\n",
        "    y = pd.to_numeric(y, errors='coerce')  # S'assurer que y est numérique\n",
        "    if y.isna().sum() > 0:\n",
        "        print(f\"Attention : {y.isna().sum()} étiquettes invalides après conversion en numérique.\")\n",
        "        df_encoded = df_encoded[~y.isna()]\n",
        "        X = df_encoded.drop(columns=[label_column])\n",
        "        y = df_encoded[label_column]\n",
        "\n",
        "    # Vérification et conversion de la matrice encodée en csr_matrix\n",
        "    if isinstance(X, csc_matrix):\n",
        "        print(\"Conversion de X en csr_matrix...\")\n",
        "        X = X.tocsr()  # Conversion de csc_matrix en csr_matrix\n",
        "    elif not isinstance(X, csr_matrix):\n",
        "        print(\"Conversion de X en csr_matrix...\")\n",
        "        X = csr_matrix(X)  # Conversion explicite si nécessaire\n",
        "    \n",
        "    # Vérifier si la conversion a réussi\n",
        "    if isinstance(X, csr_matrix):\n",
        "        print(f\"X est maintenant une matrice csr_matrix de type {type(X)}\")\n",
        "    else:\n",
        "        print(f\"X n'est pas une csr_matrix. Conversion en dense...\")\n",
        "        X = np.array(X)  # Conversion en dense si nécessaire\n",
        "    \n",
        "    # Appliquer un modèle Random Forest\n",
        "    model = RandomForestClassifier(random_state=42, n_estimators=100)  # Ajustez les paramètres comme nécessaire\n",
        "    try:\n",
        "        model.fit(X, y)\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de l'entraînement du modèle : {e}\")\n",
        "        raise e\n",
        "\n",
        "    # Calculer l'accuracy sur l'ensemble d'entraînement\n",
        "    y_pred = model.predict(X)\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    print(f\"Accuracy du modèle sur l'ensemble d'entraînement : {accuracy:.4f}\")\n",
        "    \n",
        "    # Sauvegarder le modèle\n",
        "    if save_model:\n",
        "        joblib.dump(model, model_file)\n",
        "\n",
        "    return model, accuracy\n",
        "\n",
        "# Appel de la fonction d'entraînement\n",
        "try:\n",
        "    model, accuracy = train_model_without_smote(df_encode)\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors de l'entraînement du modèle : {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\colin\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention : 39646 valeurs manquantes dans 'Label'.\n",
            "Conversion de X en csr_matrix...\n",
            "X est maintenant une matrice csr_matrix de type <class 'scipy.sparse._csr.csr_matrix'>\n",
            "Accuracy du modèle sur l'ensemble d'entraînement : 0.9995\n",
            "Avertissement : La colonne 'Label' n'existe pas dans le fichier de test. Une colonne fictive 'Label' a été ajoutée.\n",
            "Forme des données encodées : (216815, 383)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\colin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lignes comportant des anomalies :\n",
            "         Hostname Process\n",
            "164375  hilbert19    sshd\n",
            "164376  hilbert19    sshd\n",
            "164461  hilbert19    sshd\n",
            "164462  hilbert19    sshd\n",
            "164515  hilbert19    sshd\n",
            "...           ...     ...\n",
            "165332  hilbert19    sshd\n",
            "165333  hilbert19    sshd\n",
            "165334  hilbert19    sshd\n",
            "165335  hilbert19    sshd\n",
            "165336  hilbert19    sshd\n",
            "\n",
            "[696 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Étape 4 : Test du modèle\n",
        "def test_model(test_file, preprocessing_job='preprocessing_pipeline.joblib', model_file='random_forest_model.joblib'):\n",
        "    # a. Charger le fichier de test\n",
        "    df_all = pd.read_csv(test_file)\n",
        "\n",
        "    # Vérifier si la colonne 'Label' existe dans le fichier de test\n",
        "    if 'Label' not in df_all.columns:\n",
        "        print(\"Avertissement : La colonne 'Label' n'existe pas dans le fichier de test. Une colonne fictive 'Label' a été ajoutée.\")\n",
        "        df_all['Label'] = 0  # Ajouter une colonne fictive 'Label' avec une valeur par défaut (0) si elle n'existe pas\n",
        "    else:\n",
        "        print(\"La colonne 'Label' existe dans le fichier de test.\")\n",
        "\n",
        "    # b. Charger le prétraitement\n",
        "    encoder = joblib.load(preprocessing_job)\n",
        "\n",
        "    # Synchroniser les colonnes (s'assurer que df_all a les bonnes colonnes pour l'encodage)\n",
        "    columns_to_remove = set(df_all.columns) - set(encoder.feature_names_in_)\n",
        "    df_all = df_all.drop(columns=columns_to_remove, errors='ignore')\n",
        "    \n",
        "    # Ajouter les colonnes manquantes pour que df_all corresponde à encoder.feature_names_in_\n",
        "    missing_columns = set(encoder.feature_names_in_) - set(df_all.columns)\n",
        "    for col in missing_columns:\n",
        "        df_all[col] = 0  # Ou une autre valeur par défaut\n",
        "\n",
        "    # Appliquer le prétraitement\n",
        "    encoded_features = encoder.transform(df_all)\n",
        "\n",
        "    # Vérification de la forme des données encodées\n",
        "    print(f\"Forme des données encodées : {encoded_features.shape}\")\n",
        "    \n",
        "    # Vérifier si le résultat est dense ou sparse\n",
        "    if hasattr(encoded_features, 'toarray'):\n",
        "        encoded_features = encoded_features.toarray()\n",
        "\n",
        "    # Si la forme des données est correcte, on peut créer un DataFrame\n",
        "    df_all_encode = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out())\n",
        "    \n",
        "    # Supprimer la colonne encodée `Label` avant la prédiction\n",
        "    if 'Label' in df_all_encode.columns:\n",
        "        df_all_encode = df_all_encode.drop(columns=['Label'])\n",
        "\n",
        "    # c. Charger le modèle\n",
        "    model = joblib.load(model_file)\n",
        "\n",
        "    # d. Prédire les anomalies\n",
        "    predictions = model.predict(df_all_encode)\n",
        "\n",
        "    # Renvoyer les lignes correspondant aux anomalies\n",
        "    anomalies = df_all[predictions == 1]  # RandomForestClassifier marque les anomalies avec 1\n",
        "    \n",
        "    return anomalies\n",
        "\n",
        "# Tester le modèle sur un fichier de test\n",
        "anomalies = test_model('all_logs.csv')\n",
        "print(\"Lignes comportant des anomalies :\")\n",
        "print(anomalies)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
